{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "# from torchtext.datasets import text_classification\n",
    "# NGRAMS = 2\n",
    "# import os\n",
    "# if not os.path.isdir('./.data'):\n",
    "#     os.mkdir('./.data')\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "# BATCH_SIZE = 16\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_one_xtrain(sample):\n",
    "    result=[]\n",
    "    summ=0\n",
    "#     if len(sample.split())<200:\n",
    "#         X=sample+\" \"\n",
    "#         X=X*int(200/len(X.split()))\n",
    "#         sample=X\n",
    "    for word in sample.split():\n",
    "#         for ch in word:\n",
    "    \n",
    "        summ+=1\n",
    "        try: \n",
    "#             feature=np.sum(model.wv[word])\n",
    "            result.append(tokenizer[word])\n",
    "        except:\n",
    "                feature=0\n",
    "        if summ==50:\n",
    "            return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_all_xtrain(samples):\n",
    "    res=[]\n",
    "    for sample in samples:\n",
    "        s=get_feature_from_one_xtrain(sample)\n",
    "\n",
    "#         if(len(s)<50):\n",
    "            \n",
    "#             for i in range(50-len(s)):\n",
    "#                 x=np.zeros(300,dtype=np.float64)\n",
    "#                 s.append(0)\n",
    "            \n",
    "# #         x=np.zeros(20,dtype=np.float64)\n",
    "# #         for i in range(20):\n",
    "# #             if(i <len(s)):\n",
    "                \n",
    "# #                 x[i]=s[i]\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=['__label__Sarcasm','__label__Positive','__label__Negative','__label__Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=pd.read_csv('all_data_sentiment1.csv',names=['label','data'])\n",
    "X_Train,Y_Train=data_train['data'],data_train['label']\n",
    "all_sentences=alldata['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__Sarcasm': 0,\n",
       " '__label__Positive': 1,\n",
       " '__label__Negative': 2,\n",
       " '__label__Neutral': 3}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictem={}\n",
    "for i in range(len(types)):\n",
    "    dictem[types[i]]=i\n",
    "dictem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train=get_feature_from_all_xtrain(all_sentences)\n",
    "Y_Train=[dictem[i] for i in Y_Train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "allword=[]\n",
    "for s in all_sentences:\n",
    "    for word in s.split():\n",
    "        allword.append(word)\n",
    "\n",
    "f=set(allword)\n",
    "len(f)\n",
    "tokenizer=dict((word,index) for index,word in enumerate(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=[(int(Y_Train[i]),torch.tensor(X_Train[i]).to(torch.int64))for i in range(len(Y_Train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, tensor([26835, 40281, 12522])), (2, tensor([35835, 37913,  7205, 16891, 34720, 36219, 30478, 30920, 46253, 31409,\n",
      "        45949, 36223,  6849,  5418, 22416, 32964, 34013, 33152, 42863])), (2, tensor([32677, 12574, 10477, 42148, 37141, 30229, 45656,  6641, 32936, 23815,\n",
      "        32549, 43542])), (2, tensor([ 3670, 23268,  7549, 36690])), (2, tensor([14806, 17167, 28542, 35611, 37504, 21593, 16817, 23147, 18175, 13160,\n",
      "        23109, 23169,  4013, 14169, 33861,   500, 14085,   547, 23109,   510,\n",
      "        29384, 16381, 45246, 42667, 23109, 36361, 18010, 23308,  5351, 17115,\n",
      "        35650])), (2, tensor([25632, 37141, 22317, 39137, 24298, 27456,  9028, 11863, 14795, 48672,\n",
      "        17268, 33983,  7029, 46226])), (2, tensor([ 1796, 39791,  8434, 31324, 33274,  4385, 11736, 36317, 21563, 26452,\n",
      "         8695, 43112, 29266, 17252, 24679, 11994, 14255, 12830, 32685, 12763])), (2, tensor([37700, 30228, 45333,  4360, 45797, 26712,  4599])), (2, tensor([13868, 17192,  6975, 44093, 35049, 33612, 46727, 11140, 24597, 29043,\n",
      "        32753, 14169,  4205, 48445, 42938, 32753, 18324, 10707,  2528, 11967,\n",
      "        16352, 46818, 29129,  1802, 41838, 43796, 26254, 13868, 31568, 23455,\n",
      "        14660, 28542,  7350,  5351, 27412, 44449, 29204,  4602,  1345, 16053,\n",
      "        19801,   381, 11144, 13996, 33548, 16545, 43319, 39739,  3926, 31069])), (2, tensor([44457, 11364, 45236, 12162, 31013, 24504, 44990, 44457,  8017, 43171,\n",
      "        36741, 27414, 44990, 44542,  7545, 30920, 32190,  7546, 13165,  9107]))]\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:10])\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "\n",
    "            \n",
    "     \n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE =len(tokenizer)+1\n",
    "EMBED_DIM = 80\n",
    "NUN_CLASS = len(dictem.keys())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextSentiment(\n",
       "  (embedding): EmbeddingBag(49375, 80, mode=mean)\n",
       "  (fc): Linear(in_features=80, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=16, shuffle=True,collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=16, collate_fn=generate_batch)\n",
    "    i=0\n",
    "    for text, offsets, cls in data:\n",
    "        i+=1\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "#             print(cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0173(train)\t|\tAcc: 93.4%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 92.9%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0122(train)\t|\tAcc: 95.6%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 92.5%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0100(train)\t|\tAcc: 96.9%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 92.1%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0085(train)\t|\tAcc: 97.7%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0076(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 91.7%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "train_len = int(len(train_dataset) * 0.80)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    " \n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__Negative'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    output = model(torch.tensor(get_feature_from_all_xtrain([sentence])[0]), torch.tensor([0]))\n",
    "    return types[output.argmax(1).item()]\n",
    "predict(all_sentences[51])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5019"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(sub_valid_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  لازم تصير مخالفه بكب زباله'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Train[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
